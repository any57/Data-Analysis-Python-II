{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw2LIXAuKh1m",
        "outputId": "63fd36ce-eb6b-46a5-d764-b5b59b6b9884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymystem3 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pymystem3) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 33.0 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=d4802cb1ca75998eaf5dcc4eaffc451be049d309cebce419550b5ff138b41880\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines) (22.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines) (4.1.1)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-3.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install pymystem3\n",
        "\n",
        "!pip install pymorphy2\n",
        "\n",
        "!pip install jsonlines\n",
        "\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "import pymorphy2\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import json\n",
        "\n",
        "import jsonlines\n",
        "\n",
        "import string\n",
        "\n",
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with opening the text and converting it's type to string:"
      ],
      "metadata": {
        "id": "C0kJy5LWR9wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('capote.txt', 'r') as file:\n",
        "    text_in = file.read().rstrip()\n",
        "\n",
        "text1 = text_in.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "pattern = r'[0-9]'\n",
        "text = re.sub(pattern, '', text1)"
      ],
      "metadata": {
        "id": "CmMRr08aSQuo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we lemmatize the text:"
      ],
      "metadata": {
        "id": "F6rBV2EaK_bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = Mystem()\n",
        "lemmas = m.lemmatize(text)\n",
        "\n",
        "text_out = open('mystem_capote.txt','w')\n",
        "text_out.write(''.join(lemmas))\n",
        "text_out.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "7Chp84aoK-sv",
        "outputId": "59cd81c5-be27-4215-c506-5f68ff2673bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expected object or value",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0450ea92767f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtext_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mystem_capote.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mneed_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    294\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mrd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procout_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procout_no\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         raise RuntimeError(\"Problem has been occured. Current state:\\ntext:\\n%r\\nout:\\n%r\\nsio:\\n%r\" %\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we tokenize it:"
      ],
      "metadata": {
        "id": "wWqQYpodLKxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "KQLhNOBALN1Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we analyze the resulting words:"
      ],
      "metadata": {
        "id": "n_8eMcsALUMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "morph = MorphAnalyzer()\n",
        "\n",
        "i = 0\n",
        "lines = []\n",
        "\n",
        "while i < len(tokens):\n",
        "    ana = morph.parse(tokens[i])\n",
        "    current = ana[0]\n",
        "    d = {'lemma': current.normal_form, 'word': current.word, 'pos': current.tag.POS}\n",
        "    lines.append(d)\n",
        "    i += 1\n",
        "\n",
        "with jsonlines.open('pymorphy_capote.jsonl', mode='w') as writer:\n",
        "    for d in lines:\n",
        "        writer.write(d)"
      ],
      "metadata": {
        "id": "DLej2dx1LWnd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) What percentage constitutes each pos? (E.g., for the verb, the number of verbs divided by the total number of words)**\n",
        "\n",
        "To answer this question we need to make a list with every pos and it's number of entrances:"
      ],
      "metadata": {
        "id": "Q9h-iitnRxjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_pos = []\n",
        "\n",
        "for d in lines:\n",
        "    pos = d.get('pos')\n",
        "    list_of_pos.append(pos)\n",
        "\n",
        "print(Counter(list_of_pos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSy915n9RyB9",
        "outputId": "23cfe853-9d2a-4573-8be5-7f6f3c3c20cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'NOUN': 5108, 'VERB': 3389, 'NPRO': 2877, 'CONJ': 2363, 'PREP': 2274, 'ADJF': 2032, 'ADVB': 1429, 'PRCL': 1320, 'INFN': 670, 'GRND': 221, 'ADJS': 152, 'NUMR': 144, 'PRTF': 139, 'PRED': 127, 'COMP': 115, 'PRTS': 99, None: 88, 'INTJ': 40})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here is the number of all words in the text:"
      ],
      "metadata": {
        "id": "lxfge3McMkZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV2OfMvhS9fD",
        "outputId": "c050b82a-e06b-498d-8682-f7526c2c5d99"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all we need is to print out the percentage for every pos (I decided to display 3 digits after the dot because otherwise percentage of some pos comes out as 0):"
      ],
      "metadata": {
        "id": "bOplZVmXTgAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = [[x,list_of_pos.count(x)] for x in set(list_of_pos)]\n",
        "\n",
        "for i in y:\n",
        "    print(i[0], \" = \", round(i[1]/len(lines), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCQ_I_6bTpLU",
        "outputId": "d9ce727e-7398-4fc1-ceab-87192423266b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRTS  =  0.004\n",
            "PREP  =  0.101\n",
            "ADVB  =  0.063\n",
            "None  =  0.004\n",
            "NOUN  =  0.226\n",
            "ADJS  =  0.007\n",
            "PRTF  =  0.006\n",
            "PRCL  =  0.058\n",
            "INTJ  =  0.002\n",
            "NUMR  =  0.006\n",
            "INFN  =  0.03\n",
            "GRND  =  0.01\n",
            "ADJF  =  0.09\n",
            "PRED  =  0.006\n",
            "VERB  =  0.15\n",
            "COMP  =  0.005\n",
            "NPRO  =  0.127\n",
            "CONJ  =  0.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The strange category 'None' represents all the numbers, foreign words, etc. -- I decided to keep them since they are a legit part of text (and either way there are not so many of these).\n",
        "\n",
        "---\n",
        "\n",
        "**2) Print out top-20 verbs and adverbs.**\n",
        "\n",
        "To answer this question we need to make a list with of every verb and another list of every adverb (it's not clear whether I need to prin out a top-20 of verbs and adverbs or a top-20 of verbs and top-20 of adverbs so I decided to do to do separate tops) and then to sort out top-20:"
      ],
      "metadata": {
        "id": "rpmwOK9efZ7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import k0\n",
        "list_of_verbs = []\n",
        "list_of_adverbs = []\n",
        "\n",
        "for d in lines:\n",
        "    if d[\"pos\"] == 'VERB':\n",
        "        verb = d[\"lemma\"]\n",
        "        list_of_verbs.append(verb)\n",
        "\n",
        "print(Counter(list_of_verbs).most_common(20))\n",
        "\n",
        "for d in lines:\n",
        "    if d[\"pos\"] == 'ADVB':\n",
        "        adverb = d[\"lemma\"]\n",
        "        list_of_adverbs.append(adverb)\n",
        "\n",
        "print(Counter(list_of_adverbs).most_common(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzIq1uNMla6o",
        "outputId": "3818ebc4-205c-491a-f3a9-cde9bf72842f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('быть', 293), ('сказать', 146), ('мочь', 106), ('знать', 69), ('говорить', 49), ('хотеть', 47), ('любить', 40), ('казаться', 39), ('думать', 36), ('стать', 33), ('голайтлить', 30), ('видеть', 27), ('спросить', 22), ('дать', 19), ('идти', 19), ('жить', 18), ('взять', 18), ('стоять', 17), ('увидеть', 17), ('стоить', 16)]\n",
            "[('только', 72), ('ещё', 68), ('там', 47), ('уже', 40), ('потому', 36), ('потом', 35), ('где', 34), ('теперь', 33), ('очень', 30), ('пока', 30), ('здесь', 20), ('тут', 20), ('далее', 20), ('всегда', 19), ('совсем', 19), ('ничего', 17), ('снова', 16), ('тогда', 16), ('несколько', 15), ('вдруг', 15)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next task is as follows:\n",
        "**Find top-25 bigrams and trigrams for your text (use nltk.bigrams), use only lemmas, get rid of the punctuation. Comment shortly on the results.**"
      ],
      "metadata": {
        "id": "6RM327UsxmTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mystem_capote.txt', 'r') as file:\n",
        "    text_new = file.read().rstrip()\n",
        "\n",
        "bigrm = list(nltk.bigrams(text_new.split()))\n",
        "print(Counter(bigrm).most_common(25))\n",
        "\n",
        "trigrm = list(nltk.trigrams(text_new.split()))\n",
        "print(Counter(trigrm).most_common(25))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtSk4vZZx4Q8",
        "outputId": "d9a32569-0707-47b3-a2c9-270ea872978b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('я', 'не'), 63), (('что', 'я'), 53), (('сказать', 'она'), 51), (('у', 'я'), 39), (('не', 'быть'), 37), (('и', 'я'), 34), (('потому', 'что'), 34), (('она', 'и'), 30), (('и', 'не'), 30), (('это', 'быть'), 29), (('не', 'мочь'), 29), (('она', 'не'), 29), (('сказать', 'он'), 27), (('что', 'она'), 25), (('я', 'в'), 25), (('я', 'и'), 24), (('то', 'что'), 24), (('он', 'не'), 24), (('когда', 'я'), 24), (('что', 'ты'), 24), (('джо', 'белл'), 23), (('он', 'и'), 23), (('что', 'он'), 22), (('у', 'она'), 22), (('она', 'быть'), 21)]\n",
            "[(('сказать', 'она', 'и'), 11), (('о', 'д', 'берман'), 11), (('так', 'и', 'не'), 7), (('в', 'конец', 'конец'), 7), (('что', 'я', 'не'), 7), (('сказать', 'он', 'и'), 6), (('с', 'тот', 'пора'), 6), (('к', 'то', 'же'), 5), (('один', 'из', 'они'), 5), (('бар', 'джо', 'белла'), 5), (('не', 'знать', 'что'), 5), (('я', 'за', 'рука'), 5), (('мадам', 'сапфия', 'спанелла'), 5), (('то', 'что', 'я'), 5), (('не', 'мочь', 'быть'), 5), (('я', 'не', 'мочь'), 5), (('в', 'то', 'что'), 5), (('в', 'тот', 'же'), 4), (('на', 'самый', 'дело'), 4), (('это', 'она', 'и'), 4), (('я', 'не', 'знать'), 4), (('схватывать', 'я', 'за'), 4), (('не', 'то', 'чтобы'), 4), (('то', 'чтобы', 'я'), 4), (('думать', 'что', 'я'), 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see practically all bigrams are deprived of sense on their own but trigrams look more interesting: for example one of the most common trigrams is (**в**, **конец**, **конец**) which clearly refers to **в конце концов** -- it says a lot about the text guess. Another popular trigram is бар Джо Белла -- this one speaks for itself too.\n",
        "\n",
        "---\n",
        "\n",
        "The last task is as follows:\n",
        "**Take 3-8 sentences from the original text and substitute some morphological information, for example, change the tense of the verbs, the number of the nouns, e.g, the original Слон подарил мартышке цветы should become Слоны подарят мартышкам цветок.**"
      ],
      "metadata": {
        "id": "3v3CuWCRzKVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extra = 'Однажды поздно ночью я проснулся от того, что мистер Юниоши что-то кричал в пролет лестницы. Он жил на верхнем этаже, и голос его, строгий и сердитый, разносился по ' +\\\n",
        "'всему дому: - Мисс Голайтли! Я должен протестовать! Ребячливый, беззаботно-веселый голос ответил снизу: - Миленький, простите! Я опять потеряла этот дурацкий ключ. ' +\\\n",
        "'- Пожалуйста, не надо звонить в мой звонок. - Пожалуйста, сделайте себе ключ! - Да я их всё время теряю. - Я работаю, я должен спать, - кричал мистер Юниоши. - А вы всё ' +\\\n",
        "'время звоните в мой звонок! - Миленький вы мой, ну зачем вы сердитесь? Я больше не буду. Пожалуйста, не сердитесь. - Голос приближался, она поднималась по лестнице.'"
      ],
      "metadata": {
        "id": "NT--ibC11i2y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we tokenize the text in order to deal with separate words:"
      ],
      "metadata": {
        "id": "2-_1C-wMT0Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(extra)\n",
        "\n",
        "m = Mystem()\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "extra_new = []"
      ],
      "metadata": {
        "id": "eBv4yGJ0Odsf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we transform the text to the text of the same meaning but in plural form.\n",
        "\n",
        "The general idea is simple: we need to switch nouns, pronouns, verbs, adjectives (including the short ones) to their plural forms and leave the others as they are. Also in case of verbs we need to maintain their tense and mood.\n",
        "\n",
        "The code is devided in 4 major chunks:\n",
        "\n",
        "**1. Exceptions.**\n",
        "\n",
        "There are 3 kinds of issues with some words -- proper nouns, suiting parses and something I can't even name. **The first issue** is that proper nouns (in my case these are *Юниоши*, *Голайтли*) don't have plural forms but there seems to be no way to distinguish them from common nouns automatically. **The second issue** is that from time to time the most probable parse (i.e. ana[0]) is not the correct one (in my case these are *того*, *лестницы*) -- and again I can't think of a way to automatically verify whether ana[0] is the correct parse for each word. As for **the third issue** -- sometimes there are words that can be changed to their plural forms but shouldn't be in this particular case (in my case this is *время*) -- now this could be solved with syntactic analysis so it is worth looking into the issue;\n",
        "\n",
        "**2. Pronouns**\n",
        "\n",
        "The problem with pronouns is they **should** be changed to the plural forms but at the same time, for example, *мы* is not a plural form for *я* strictly speaking. For this reason I had no other way but to write down every option manualy;\n",
        "\n",
        "**3. Verbs, nouns, adjectives, short adjectives**\n",
        "\n",
        "In this chunk of code everything is quite straightforward: we switch words fitting the condition to their plural forms (at the same time keeping same tense and mood for verbs);\n",
        "\n",
        "**4. The rest**\n",
        "\n",
        "Every other token should be left as it is -- which is exactly what I do!\n",
        "\n",
        "**And here we go:**"
      ],
      "metadata": {
        "id": "3eURFOYkT8uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tokens:\n",
        "    ana = morph.parse(i)\n",
        "    wrd = ana[0]\n",
        "    #several exception that I couldn't figure out\n",
        "    if wrd.word == 'юниоши' or wrd.word == 'голайтли' or wrd.word == 'время' or wrd.word == 'того' or wrd.word == 'лестницы':\n",
        "        if wrd.word == 'юниоши':\n",
        "            text_new.append('Юниоши')\n",
        "        elif wrd.word == 'голайтли':\n",
        "            text_new.append('Голайтли')\n",
        "        elif wrd.word == 'время':\n",
        "            text_new.append('время')\n",
        "        elif wrd.word == 'того':\n",
        "            text_new.append('того')\n",
        "        elif wrd.word == 'лестницы':\n",
        "            text_new.append('лестниц')\n",
        "    else:\n",
        "        #pronouns that need to be switched to plural forms manualy \n",
        "        if wrd.tag.POS == 'NPRO':\n",
        "            if wrd.word == 'я':\n",
        "                pl = 'мы'\n",
        "            elif wrd.word == 'ты':\n",
        "                pl = 'вы'\n",
        "            elif wrd.word == 'он' or wrd.word == 'она' or wrd.word == 'оно':\n",
        "                pl = 'они'\n",
        "            elif wrd.word == 'свой' or wrd.word == 'своя' or wrd.word == 'своё':\n",
        "                pl = 'свои'\n",
        "            elif wrd.word == 'мой' or wrd.word == 'моя' or wrd.word == 'моё':\n",
        "                pl = 'наши'\n",
        "            elif wrd.word == 'твой' or wrd.word == 'твоя' or wrd.word == 'твоё':\n",
        "                pl = 'ваши'\n",
        "            elif wrd.word == 'его' or wrd.word == 'её':\n",
        "                pl = 'их'\n",
        "            elif wrd.word == 'себе':\n",
        "                pl = 'себе'\n",
        "            else:\n",
        "                #pronouns already in plural form\n",
        "                pl = wrd.word\n",
        "            text_new.append(pl)\n",
        "        else:\n",
        "            #major text change happens here\n",
        "            if wrd.tag.POS == 'VERB' or wrd.tag.POS == 'NOUN' or wrd.tag.POS == 'ADJF' or wrd.tag.POS == 'ADJS':\n",
        "                #paying attention to verbs and their tags\n",
        "                if wrd.tag.POS == 'VERB':\n",
        "                    if wrd.tag.tense == 'past':\n",
        "                        pl = wrd.inflect({'plur', 'past'})\n",
        "                    elif wrd.tag.tense == 'pres':\n",
        "                        pl = wrd.inflect({'plur', 'pres'})\n",
        "                    elif wrd.tag.tense == 'futr':\n",
        "                        pl = wrd.inflect({'plur', 'futr'})\n",
        "                    elif wrd.tag.mood == 'impr' and wrd.tag.number == 'sing':\n",
        "                        pl = wrd.inflect({'impr', 'plur'})\n",
        "                    elif wrd.tag.mood == 'impr' and wrd.tag.number == 'plur':\n",
        "                        pl = wrd.inflect({'impr', 'plur'})\n",
        "                else:\n",
        "                    #nouns and adjectives are less demanding\n",
        "                    pl = wrd.inflect({'plur'})\n",
        "                text_new.append(pl.word)\n",
        "            else:\n",
        "                #and finally all the rest tokens need no change\n",
        "                pl = wrd.word\n",
        "                text_new.append(wrd.word)"
      ],
      "metadata": {
        "id": "CppQEpKcOib-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now I just need to print out the text and the result of the program:"
      ],
      "metadata": {
        "id": "zBJ4rpILgKlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Text:', extra, end = '\\n\\n')\n",
        "print(' '.join(extra_new), end = '\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1w0D1IsOn0p",
        "outputId": "e6864fd3-d191-464e-ee51-61a48ccff566"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Однажды поздно ночью я проснулся от того, что мистер Юниоши что-то кричал в пролет лестницы. Он жил на верхнем этаже, и голос его, строгий и сердитый, разносился по всему дому: - Мисс Голайтли! Я должен протестовать! Ребячливый, беззаботно-веселый голос ответил снизу: - Миленький, простите! Я опять потеряла этот дурацкий ключ. - Пожалуйста, не надо звонить в мой звонок. - Пожалуйста, сделайте себе ключ! - Да я их всё время теряю. - Я работаю, я должен спать, - кричал мистер Юниоши. - А вы всё время звоните в мой звонок! - Миленький вы мой, ну зачем вы сердитесь? Я больше не буду. Пожалуйста, не сердитесь. - Голос приближался, она поднималась по лестнице.\n",
            "\n",
            "однажды поздно ночью мы проснулись от того , что мистеры Юниоши что-то кричали в пролёты лестниц . они жили на верхних этажах , и голоса их , строгие и сердитые , разносились по всем домам : - мисс Голайтли ! мы должны протестовать ! ребячливые , беззаботно-весёлые голоса ответили снизу : - миленькие , простите ! мы опять потеряли эти дурацкие ключи . - пожалуйста , не надо звонить в мои звонки . - пожалуйста , сделайте себе ключи ! - да мы их всё время теряем . - мы работаем , мы должны спать , - кричали мистеры Юниоши . - а вы всё время звоните в мои звонки ! - миленькие вы мои , ну зачем вы сердитесь ? мы больше не будем . пожалуйста , не сердитесь . - голоса приближались , они поднимались по лестницам .\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
